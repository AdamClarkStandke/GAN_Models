{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepConvolutionalGan(DCGAN).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQOLvaUlLnmgiY+cbWtyS1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aCStandke/GAN_Models/blob/main/DeepConvolutionalGan(DCGAN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Convolutional GAN (DCGAN)\n",
        "\n",
        "Implementation of the Deep Convolutional GAN as detailed in Unsurpervised Representation Learning with Deep Convolutional Generative Adversaial Networks, [DCGAN](https://arxiv.rg/abs/15511.06434) and detailed in page 196 of the book Advanced Deep Learning with Python \n",
        "\n"
      ],
      "metadata": {
        "id": "3yfjd1u52iJU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gs22OI-P18CV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Dropout, Input, Dense, Reshape, Flatten\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import adam_v2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator\n"
      ],
      "metadata": {
        "id": "DO3VDbykG7zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function that builds the generator\n",
        "def build_generator(latent_input):\n",
        "  model = Sequential([\n",
        "                      # first fully connected layer to take in 1D latent vector of the generator \n",
        "                      Dense(7*7*256, use_bias=False, input_shape=latent_input.shape[1:]),\n",
        "                      # applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n",
        "                      BatchNormalization(), \n",
        "                      LeakyReLU(), \n",
        "                      Reshape((7, 7, 256)),\n",
        "                      # first layer of upsampeling(i.e. deconvolution) of the 1D latent vector/input  \n",
        "                      Conv2DTranspose(filters=128, kernel_size=(5,5), strides=(1,1), padding='same', use_bias=False), \n",
        "                      BatchNormalization(), \n",
        "                      LeakyReLU(), \n",
        "                      # second layer of upsampeling(i.e. deconvolution) in which the volume depth is reduced\n",
        "                      Conv2DTranspose(filters=64, kernel_size=(5,5), strides=(2,2), padding='same', use_bias=False), \n",
        "                      BatchNormalization(), \n",
        "                      LeakyReLU(), \n",
        "                      # third layer upsampeling(i.e. deconvolution) in which the volume depth is reduced\n",
        "                      Conv2DTranspose(filters=1, kernel_size=(5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh'), \n",
        "\n",
        "  ])\n",
        "\n",
        "  # forward propogation of model\n",
        "  generated = model(latent_input)\n",
        "  return Model(z, generated)"
      ],
      "metadata": {
        "id": "ymvL5uTZG5FT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator"
      ],
      "metadata": {
        "id": "QOalxX9NfxtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential([\n",
        "                     # first layers of discriminator network\n",
        "                     Conv2D(filters=64, kernel_size=(5, 5), strides=(2,2), padding='same', input_shape = (28, 28, 1)),\n",
        "                     LeakyReLU(),\n",
        "                     Dropout(0.3), \n",
        "                     # second layers of discriminator network\n",
        "                     Conv2D(filters=128, kernel_size=(5, 5), strides=(2,2), padding='same'),\n",
        "                     LeakyReLU(),\n",
        "                     Dropout(0.3),\n",
        "                     Flatten(), \n",
        "                     Dense(1, activation='sigmoid'),                   \n",
        "                    ])\n",
        "  image = Input(shape=(28, 28, 1))\n",
        "  output = model(image)\n",
        "  return Model(image, output)"
      ],
      "metadata": {
        "id": "9fY-P644fwwe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Function"
      ],
      "metadata": {
        "id": "NMrS-ueUqYoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(generator, discriminator, combined, steps, batch_size):\n",
        "  # loading the dataset\n",
        "  (x_train, _), _ = fashion_mnist.load_data()\n",
        "  # Rescale in [-1,1] interval\n",
        "  x_train = (x_train.astype(np.float32)-127.5)/127.5\n",
        "  x_train = np.expand_dims(x_train, axis=-1)\n",
        "  # Discriminator ground truths for real and fake images\n",
        "  real = np.ones((batch_size, 1))\n",
        "  fake = np.zeros((batch_size, 1))\n",
        "\n",
        "  latent_dim = generator.input_shape[1]\n",
        "\n",
        "  for step in range(steps):\n",
        "    # first train the discriminator\n",
        "\n",
        "      # select a random batch of real images from the uninform distribution\n",
        "      real_images = x_train[np.random.randint(0, x_train.shape[0], batch_size)]\n",
        "\n",
        "      # generate a random batch of noise from the normal distribution for the generator \n",
        "      noise = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n",
        "\n",
        "      # generate a bath of new images using the generator\n",
        "      generated_images = generator.predict(noise)\n",
        "\n",
        "      # discriminator loss on real images\n",
        "      discriminator_real_loss = discriminator.train_on_batch(real_images, real)\n",
        "      # discriminator loss on fake images\n",
        "      discriminator_fake_loss = discriminator.train_on_batch(generated_images, fake)\n",
        "      # discriminator total loss\n",
        "      discriminator_loss = 0.5 * np.add(discriminator_real_loss, discriminator_fake_loss)\n",
        "\n",
        "    # Second train the generator\n",
        "\n",
        "      # generate latent vector z from the normal distribution\n",
        "      noise = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n",
        "      # generator loss: As noted in the book the real images are combined with the latent vector to maximize the discriminator loss\n",
        "      generator_loss = combined.train_on_batch(noise, real)\n",
        "\n",
        "      # Display progress\n",
        "      print(\"%d [Discriminator loss: %.4f%%, acc.: %.2f%%] [Generator loss: %.4f%%]\" % (step, discriminator_loss[0], 100*discriminator_loss[1], generator_loss))\n",
        "      "
      ],
      "metadata": {
        "id": "M3iBRkbtqVq9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Generated Images"
      ],
      "metadata": {
        "id": "qVghG2FZuCoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_generated_images(generator):\n",
        "  n = 10\n",
        "  image_size = 28\n",
        "\n",
        "  # big array containing all images\n",
        "  figure = np.zeros((image_size * n, image_size* n))\n",
        "\n",
        "  latent_dim = generator.input_shape[1]\n",
        "\n",
        "  # 100 latent vectors sampled from a normal distribution\n",
        "  noise = np.random.normal(loc=0, scale=1, size=(n * n, latent_dim)) \n",
        "\n",
        "  # generate the images \n",
        "  generated_images = generator.predict(noise)\n",
        "\n",
        "  # fill the big array with images\n",
        "  for i in range(n):\n",
        "     for j in range(n):\n",
        "       slice_i = slice(i*image_size, (i+1)*image_size)\n",
        "       slice_j = slice(j*image_size, (j+1)*image_size)\n",
        "\n",
        "  # plot the results\n",
        "  plt.figure(figsize=(6, 5))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(figure, cmap='gray', vmin=0, vmax=255)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "KlIU-xW3uA4I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 64\n",
        "\n",
        "# build the generator\n",
        "z = Input(shape=(latent_dim))\n",
        "generator = build_generator(z)\n",
        "generated_image = generator(z)\n",
        "\n",
        "optimizer = adam_v2.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "# build the discriminator\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Key!!: only training the generator for the combined model\n",
        "discriminator.trainable = False\n",
        "\n",
        "# discriminator deciding if image is fake or real\n",
        "real_or_fake = discriminator(generated_image)\n",
        "\n",
        "# Key!!: stacking the generator and discriminator in a combined model, and then training the generator\n",
        "combined = Model(z, real_or_fake)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "train(generator, discriminator, combined, steps=25, batch_size=100)\n",
        "\n",
        "# calling plot to see what images the generator has created from the fashion dataset\n",
        "plot_generated_images(generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "jeGWlYFL9J3s",
        "outputId": "d971cb59-6fa4-4a34-9dab-e013263ff3ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [Discriminator loss: 0.7005%, acc.: 25.50%] [Generator loss: 0.6759%]\n",
            "1 [Discriminator loss: 0.6540%, acc.: 49.00%] [Generator loss: 0.6176%]\n",
            "2 [Discriminator loss: 0.6090%, acc.: 50.00%] [Generator loss: 0.5390%]\n",
            "3 [Discriminator loss: 0.5658%, acc.: 50.00%] [Generator loss: 0.4532%]\n",
            "4 [Discriminator loss: 0.5324%, acc.: 50.00%] [Generator loss: 0.3565%]\n",
            "5 [Discriminator loss: 0.4874%, acc.: 50.00%] [Generator loss: 0.2645%]\n",
            "6 [Discriminator loss: 0.4520%, acc.: 52.00%] [Generator loss: 0.1988%]\n",
            "7 [Discriminator loss: 0.4269%, acc.: 73.00%] [Generator loss: 0.1435%]\n",
            "8 [Discriminator loss: 0.4041%, acc.: 94.50%] [Generator loss: 0.1030%]\n",
            "9 [Discriminator loss: 0.3864%, acc.: 98.00%] [Generator loss: 0.0757%]\n",
            "10 [Discriminator loss: 0.3773%, acc.: 92.00%] [Generator loss: 0.0557%]\n",
            "11 [Discriminator loss: 0.3845%, acc.: 73.50%] [Generator loss: 0.0441%]\n",
            "12 [Discriminator loss: 0.3827%, acc.: 53.50%] [Generator loss: 0.0376%]\n",
            "13 [Discriminator loss: 0.3989%, acc.: 50.50%] [Generator loss: 0.0369%]\n",
            "14 [Discriminator loss: 0.4017%, acc.: 50.00%] [Generator loss: 0.0415%]\n",
            "15 [Discriminator loss: 0.4171%, acc.: 50.00%] [Generator loss: 0.0516%]\n",
            "16 [Discriminator loss: 0.4184%, acc.: 50.00%] [Generator loss: 0.0615%]\n",
            "17 [Discriminator loss: 0.4198%, acc.: 50.00%] [Generator loss: 0.0722%]\n",
            "18 [Discriminator loss: 0.4293%, acc.: 50.00%] [Generator loss: 0.0917%]\n",
            "19 [Discriminator loss: 0.4243%, acc.: 50.00%] [Generator loss: 0.1001%]\n",
            "20 [Discriminator loss: 0.4333%, acc.: 50.00%] [Generator loss: 0.0972%]\n",
            "21 [Discriminator loss: 0.4435%, acc.: 50.00%] [Generator loss: 0.0895%]\n",
            "22 [Discriminator loss: 0.4535%, acc.: 50.00%] [Generator loss: 0.0866%]\n",
            "23 [Discriminator loss: 0.4500%, acc.: 50.00%] [Generator loss: 0.1290%]\n",
            "24 [Discriminator loss: 0.4272%, acc.: 50.00%] [Generator loss: 0.1783%]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEeCAYAAABcyXrWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADsElEQVR4nO3ZIQ7DQAwAwTjq/7/svqABBXsgM9TEaGXJs7sXQOk+vQDwPsID5IQHyAkPkBMeICc8QO7zNJwZv3bgL7s7v2YuHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfIze6e3gF4GRcPkBMeICc8QE54gJzwADnhAXJfbPIMN0BZVzwAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}